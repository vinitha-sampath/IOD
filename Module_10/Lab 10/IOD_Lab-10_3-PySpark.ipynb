{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0wJ6ih0dGue"
   },
   "source": [
    "<div>\n",
    "<img src=https://www.institutedata.com/wp-content/uploads/2019/10/iod_h_tp_primary_c.svg width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9ROjAGE3Xwe"
   },
   "source": [
    "# Lab 10.3 PySpark for Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1x0a8wS36Dg"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "**Note**: this notebook is to be run in Google Colab on your Google Drive. It will not work locally on your computer.\n",
    "\n",
    "The purpose of this lab is to gain further exposure to cloud computing, often necessary when datasets become too large to manage on a local machine. You will learn how to work with a large dataset through the PySpark Python library with Google Colaboratory (Colab).\n",
    "\n",
    "In Google Colab, a virtual machine is automatically set up to execute your code. The maximum lifetime of such a machine is 12 hours. Note that notebooks will be disconnected from virtual machines if left idle. If this happens simple click on the Connect button to reconnect. If the kernel needs to be restarted (via the Runtime menu), variables may be lost but packages would not need to be reinstalled unless a new machine is assigned.\n",
    "\n",
    "https://research.google.com/colaboratory/faq.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Aofk5PAOjiy"
   },
   "source": [
    "Sign into colab.research.google.com and choose the Upload tab and upload this notebook.  This will automatically create a folder called \"Colab Notebooks\" in your Google Drive (if it does not already exist)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q9D2nLzvpvXG"
   },
   "source": [
    "Apache Spark is an open-source cluster-computing framework, able to work with large datasets quickly by performing in-memory caching and computation. Pyspark is a Python API for Spark commonly used to manipulate big data. For reference one useful cheat sheet is available at https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf\n",
    "\n",
    "\n",
    "Fortunately Pyspark is straightforward to setup in Google Colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 59937,
     "status": "ok",
     "timestamp": 1689050259256,
     "user": {
      "displayName": "Kirk Matheson",
      "userId": "01103899529112382706"
     },
     "user_tz": -720
    },
    "id": "0OjLwFBPiHjE",
    "outputId": "17f9ec0e-fe46-4b94-e048-da6f77b6efa0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
      "     ---------------------------------------- 0.0/317.3 MB ? eta -:--:--\n",
      "      -------------------------------------- 6.3/317.3 MB 32.2 MB/s eta 0:00:10\n",
      "     - ------------------------------------ 13.4/317.3 MB 32.3 MB/s eta 0:00:10\n",
      "     -- ----------------------------------- 20.7/317.3 MB 32.7 MB/s eta 0:00:10\n",
      "     --- ---------------------------------- 28.3/317.3 MB 33.2 MB/s eta 0:00:09\n",
      "     ---- --------------------------------- 35.4/317.3 MB 33.1 MB/s eta 0:00:09\n",
      "     ----- -------------------------------- 42.5/317.3 MB 33.4 MB/s eta 0:00:09\n",
      "     ----- -------------------------------- 49.5/317.3 MB 33.5 MB/s eta 0:00:08\n",
      "     ------ ------------------------------- 56.6/317.3 MB 33.4 MB/s eta 0:00:08\n",
      "     ------- ------------------------------ 60.6/317.3 MB 31.6 MB/s eta 0:00:09\n",
      "     -------- ----------------------------- 67.9/317.3 MB 31.8 MB/s eta 0:00:08\n",
      "     --------- ---------------------------- 75.2/317.3 MB 32.0 MB/s eta 0:00:08\n",
      "     --------- ---------------------------- 81.8/317.3 MB 31.8 MB/s eta 0:00:08\n",
      "     ---------- --------------------------- 89.1/317.3 MB 32.1 MB/s eta 0:00:08\n",
      "     ----------- -------------------------- 95.9/317.3 MB 32.2 MB/s eta 0:00:07\n",
      "     ------------ ------------------------ 103.5/317.3 MB 32.2 MB/s eta 0:00:07\n",
      "     ------------ ------------------------ 110.9/317.3 MB 32.3 MB/s eta 0:00:07\n",
      "     ------------- ----------------------- 116.7/317.3 MB 32.1 MB/s eta 0:00:07\n",
      "     -------------- ---------------------- 123.7/317.3 MB 32.0 MB/s eta 0:00:07\n",
      "     --------------- --------------------- 131.3/317.3 MB 32.3 MB/s eta 0:00:06\n",
      "     ---------------- -------------------- 138.7/317.3 MB 32.3 MB/s eta 0:00:06\n",
      "     ----------------- ------------------- 146.0/317.3 MB 32.4 MB/s eta 0:00:06\n",
      "     ----------------- ------------------- 151.8/317.3 MB 32.1 MB/s eta 0:00:06\n",
      "     ------------------ ------------------ 159.4/317.3 MB 32.2 MB/s eta 0:00:05\n",
      "     ------------------- ----------------- 166.5/317.3 MB 32.2 MB/s eta 0:00:05\n",
      "     -------------------- ---------------- 173.8/317.3 MB 32.3 MB/s eta 0:00:05\n",
      "     --------------------- --------------- 181.1/317.3 MB 32.3 MB/s eta 0:00:05\n",
      "     --------------------- --------------- 188.5/317.3 MB 32.4 MB/s eta 0:00:04\n",
      "     ---------------------- -------------- 195.8/317.3 MB 32.4 MB/s eta 0:00:04\n",
      "     ----------------------- ------------- 200.5/317.3 MB 32.3 MB/s eta 0:00:04\n",
      "     ----------------------- ------------- 205.5/317.3 MB 31.8 MB/s eta 0:00:04\n",
      "     ------------------------ ------------ 213.4/317.3 MB 31.9 MB/s eta 0:00:04\n",
      "     ------------------------- ----------- 220.7/317.3 MB 32.0 MB/s eta 0:00:04\n",
      "     -------------------------- ---------- 227.8/317.3 MB 32.0 MB/s eta 0:00:03\n",
      "     --------------------------- --------- 235.4/317.3 MB 32.1 MB/s eta 0:00:03\n",
      "     ---------------------------- -------- 242.5/317.3 MB 32.1 MB/s eta 0:00:03\n",
      "     ----------------------------- ------- 249.8/317.3 MB 32.1 MB/s eta 0:00:03\n",
      "     ------------------------------ ------ 257.4/317.3 MB 32.2 MB/s eta 0:00:02\n",
      "     ------------------------------ ------ 262.4/317.3 MB 32.0 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 270.0/317.3 MB 32.1 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 278.9/317.3 MB 32.0 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 286.3/317.3 MB 32.0 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 293.6/317.3 MB 32.0 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 301.2/317.3 MB 32.0 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 308.5/317.3 MB 32.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  316.1/317.3 MB 32.0 MB/s eta 0:00:01\n",
      "     ------------------------------------  317.2/317.3 MB 32.0 MB/s eta 0:00:01\n",
      "     ------------------------------------  317.2/317.3 MB 32.0 MB/s eta 0:00:01\n",
      "     ------------------------------------  317.2/317.3 MB 32.0 MB/s eta 0:00:01\n",
      "     ------------------------------------  317.2/317.3 MB 32.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- 317.3/317.3 MB 28.9 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting py4j\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): still running...\n",
      "  Building wheel for pyspark (setup.py): still running...\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840669 sha256=49ddd05d79ea5bb5759a88fb0aaaa5db2469359ca6046614c972c6e5b857438e\n",
      "  Stored in directory: c:\\users\\vinitha\\appdata\\local\\pip\\cache\\wheels\\07\\a0\\a3\\d24c94bf043ab5c7e38c30491199a2a11fef8d2584e6df7fb7\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark py4j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqVKUUfaPAFk"
   },
   "source": [
    "To work with Spark DataFrames we firstly need to create a Spark DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EVkw1kauisG7"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, trim, avg, round, when\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0DhoqhUfjLLR"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"populationdata\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "executionInfo": {
     "elapsed": 2791,
     "status": "ok",
     "timestamp": 1689050276938,
     "user": {
      "displayName": "Kirk Matheson",
      "userId": "01103899529112382706"
     },
     "user_tz": -720
    },
    "id": "VR8Wz35Srk2e",
    "outputId": "c78717b6-0195-4c10-94f8-b3682f851fc2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://6690a24fa7d7:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>populationdata</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb348bc9db0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fniWWP-mUHbj"
   },
   "source": [
    "## Loading the dataset and EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARXXUB6T4YyX"
   },
   "source": [
    "The dataset to be analysed in this lab shows population estimates by age and gender:\n",
    "\n",
    "   - PopMale: Male population for the individual age (thousands)\n",
    "   - PopFemale: Female population for the individual age (thousands)\n",
    "   - PopTotal: Total population for the individual age (thousands)\n",
    "\n",
    "Further details can be found at https://population.un.org/wpp/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SIwmSmYtJSce"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24927,
     "status": "ok",
     "timestamp": 1689050442200,
     "user": {
      "displayName": "Kirk Matheson",
      "userId": "01103899529112382706"
     },
     "user_tz": -720
    },
    "id": "UAEvb3tCJP5X",
    "outputId": "7ba34756-9536-4a5b-dbbf-8993f5338ae2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9PIO6ccj11y"
   },
   "outputs": [],
   "source": [
    "# run this cell after uploading the file into your Colab Notebooks folder\n",
    "df = spark.read.csv(r\"/content/drive/MyDrive/Colab Notebooks/WPP2019_PopulationBySingleAgeSex_1950-2019.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0HBFMsS4S_0y"
   },
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gjGaJckiP5Wq"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jx2B5DdBt-uv"
   },
   "source": [
    "Note that unlike Pandas this does not display a preview of the dataset, only the schema. This is because Spark performs lazy evaluation, only displaying rows when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x2oftXQoXLM5"
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yUzIy3XxMEMu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEJqR51YMH-a"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sGp29RbMIUA"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0T1d7f8ucdV"
   },
   "source": [
    "**Exercise**: How many rows does df contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Or9UO3KfkO2j"
   },
   "outputs": [],
   "source": [
    "# ANSWER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3OrPKvO1wwmF"
   },
   "outputs": [],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laKXx1YuVHzJ"
   },
   "source": [
    "This corresponds to the number of cores in a free Google Colab instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U46UOmCm-JGW"
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wbu-qsbAurU_"
   },
   "source": [
    "We use df.show() similarly to df.head() in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2gErdsL3kcMu"
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yzxQaJivXU-d"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRiwEStKu1Bj"
   },
   "source": [
    "Once again evaluation of describe is lazy, we use show() to display results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b086yRpkcFm1"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "019DYlC1Xa4f"
   },
   "outputs": [],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUaRjQT1UKQA"
   },
   "source": [
    "Let us remove rows we do not need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NGE3jFrNnKPN"
   },
   "outputs": [],
   "source": [
    "df.select('VarID', 'Variant').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImP40Am2Ubyk"
   },
   "source": [
    "As we will not work with other variant, we can safely drop these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FcUa1NXkUvuK"
   },
   "outputs": [],
   "source": [
    "df = df.drop('VarID', 'Variant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E9JqgSV8U0MO"
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FetISlkDkfte"
   },
   "outputs": [],
   "source": [
    "df.select('Location').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLp0s2DNm_9J"
   },
   "outputs": [],
   "source": [
    "df.select('Location').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGId26gpvIhA"
   },
   "source": [
    "**Exercise**: Repeat the above query, this time ordering the results by Location and using the truncate=False option to display results in full. Show all 440 results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6n-RKO9GQ9d7"
   },
   "outputs": [],
   "source": [
    "# ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4yDHFBOxvoAu"
   },
   "source": [
    "Next run the following to confirm that there are no missing values in this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ED5mlcDZWQ_y"
   },
   "outputs": [],
   "source": [
    "df.count() - df.na.drop().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vM0ibqHheNwi"
   },
   "source": [
    "## Convert types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tp9mGREhvt9b"
   },
   "source": [
    "We saw above that all columns are in the form of strings. The following cell converts some of the columns to type float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AhMt9KiAAQWm"
   },
   "outputs": [],
   "source": [
    "floatcols = ['MidPeriod', 'PopMale', 'PopFemale', 'PopTotal']\n",
    "\n",
    "for col_name in floatcols:\n",
    "    df = df.withColumn(col_name, trim(col(col_name)).cast('float'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3rr3rErWiLZ"
   },
   "source": [
    "**Exercise**: Similarly convert the four columns listed below into **integers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ccgnri3hJgk2"
   },
   "outputs": [],
   "source": [
    "intcols = ['Time', 'AgeGrp', 'AgeGrpStart', 'AgeGrpSpan']\n",
    "\n",
    "#ANSWER\n",
    "for col_name in intcols:\n",
    "    df = df.withColumn(col_name, trim(col(col_name)).cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O6S0JbDWDJM1"
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_A8nwmrlFQH"
   },
   "source": [
    "## Answering some queries about the data\n",
    "\n",
    "We use filter to select a subset of rows satisfying a True/False condition:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPrXmt9ZlPjH"
   },
   "source": [
    "Example: What was the population breakdown by age and gender in Australia in 1970?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JkwtyIjbeAR2"
   },
   "outputs": [],
   "source": [
    "df.filter((df.Location == 'Australia') & (df.Time == 1970)).show(101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utx53EQwjUSY"
   },
   "source": [
    "The select function can select a subset of columns.\n",
    "\n",
    "**Exercise**: What was the population of 45-year-old females in India in 1960?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zwxYNG2qjpft"
   },
   "outputs": [],
   "source": [
    "# ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13IpzCh23AdD"
   },
   "source": [
    "**Exercise**: Write a filter query to show the distinct locations starting with 'UN'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmgSw_z7s_-S"
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ucwk6vQ0qu63"
   },
   "source": [
    "Another example: what is the population of each location in 1950 and 2019?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C_L9r2lLq1ix"
   },
   "outputs": [],
   "source": [
    "populations_in_1950 = df.filter(df.Time == 1950)\\\n",
    "                    .groupBy('Location')\\\n",
    "                    .sum('PopTotal')\\\n",
    "                    .withColumnRenamed('sum(PopTotal)', 'Population_1950')\\\n",
    "                    .withColumn('Population_1950', round('Population_1950', 3))\\\n",
    "                    .orderBy(col('Population_1950').desc())\n",
    "populations_in_1950.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DT125gNwiBn-"
   },
   "outputs": [],
   "source": [
    "populations_in_2019 = df.filter(df.Time == 2019)\\\n",
    "                    .groupBy('Location')\\\n",
    "                    .sum('PopTotal')\\\n",
    "                    .withColumnRenamed('sum(PopTotal)', 'Population_2019')\\\n",
    "                    .withColumn('Population_2019', round('Population_2019', 3))\\\n",
    "                    .orderBy(col('Population_2019').desc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TGQ3PPvDiWu3"
   },
   "outputs": [],
   "source": [
    "populations_in_1950.join(populations_in_2019, 'Location').orderBy('Location').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUmwRcHuXxVo"
   },
   "source": [
    "**Exercise**: Which locations had the largest percentage change in population from 1950 to 2019?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZWfatPCl4vu"
   },
   "outputs": [],
   "source": [
    "# ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Amz7aJ8TpWju"
   },
   "source": [
    "**Bonus Exercise**: In 2019 which locations have the highest percentage of seniors (age 80+) relative to their total population?\n",
    "\n",
    "(Hint: if you find Japan, Greece and Italy amongst them you are right!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9aqlueYo2x1"
   },
   "outputs": [],
   "source": [
    "# ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JF2elDeHXRpY"
   },
   "source": [
    "## Population line plot\n",
    "\n",
    "In this section, we use a query to perform a data visualisation with matplotlib. We shall plot population vs year for three countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "on49bDYynXuS"
   },
   "outputs": [],
   "source": [
    "df_3countries = df[df.Location.isin(\"Australia\", \"New Zealand\", \"Singapore\")]\\\n",
    "                  .select('Location', 'MidPeriod', 'PopTotal') \\\n",
    "                  .groupBy('Location', 'MidPeriod')\\\n",
    "                  .sum('PopTotal')\\\n",
    "                  .withColumnRenamed('sum(PopTotal)', 'Population')\\\n",
    "                  .withColumn('Population', round('Population', 3))\n",
    "df_3countries.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "94YAOx4DggYX"
   },
   "outputs": [],
   "source": [
    "dataforplotting = df_3countries.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HizTgSBkYP80"
   },
   "source": [
    "Now that this dataset is of a manageable size, we convert it to a Pandas dataframe for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MTcUn1rWZw3H"
   },
   "outputs": [],
   "source": [
    "dataforplotting.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QkkQaikca5ic"
   },
   "outputs": [],
   "source": [
    "dataforplotting.pivot_table(index=['MidPeriod'], columns='Location', values='Population')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Su9Q_zRYafh"
   },
   "outputs": [],
   "source": [
    "ax = dataforplotting.pivot_table(index=['MidPeriod'], columns='Location', values='Population').plot()\n",
    "ax.set_xlabel('Year');\n",
    "ax.set_ylabel('Population in thousands');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lk7vpgzhoyyu"
   },
   "source": [
    "**Bonus Exercise**: Plot the world population of children (age 0-17) along with those of the age groups 18-39 and 40+ from 1950 to 2019. You should display three lines (one for the population of each age group vs year) on the same plot. Note that 'World' is one of the locations.\n",
    "\n",
    "Hint: one approach is to create a new column 'AgeCategory' based on AgeGrpStart using the 'when' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3seZfEDKztf1"
   },
   "outputs": [],
   "source": [
    "# ANSWER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r4TBvqSxNB0X"
   },
   "outputs": [],
   "source": [
    "agegrp_df = world_data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zvn4652v0LBB"
   },
   "outputs": [],
   "source": [
    "ax = agegrp_df.pivot_table(index=['MidPeriod'], columns='AgeCategory', values='Population').plot()\n",
    "ax.set_xlabel('Year');\n",
    "ax.set_ylabel('Population in thousands');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CjanSfzLshCY"
   },
   "source": [
    "## Population Pyramid\n",
    "\n",
    "In this section we show how a population pyramid may be created. We look at China in the year 1980."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y-kPKZoSpvu1"
   },
   "outputs": [],
   "source": [
    "china_1980 = df.filter((df.Location == 'China (and dependencies)') & (df.Time == 1980)).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7QO-QPj5-BFa"
   },
   "outputs": [],
   "source": [
    "china_1980"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nDnOzJv590KU"
   },
   "outputs": [],
   "source": [
    "china_1980.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxwmLmbpT-MU"
   },
   "source": [
    "Create age brackets in multiples of 5 - such as 0-4, 5-9, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EDNkxHWPtIBv"
   },
   "outputs": [],
   "source": [
    "lower = china_1980['AgeGrpStart'] - (china_1980['AgeGrpStart'] % 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qDdobQzVvS6O"
   },
   "outputs": [],
   "source": [
    "agebrackets = [f'{x:02d}-{(x+4):02d}' for x in lower.values]\n",
    "agebrackets[-1] = '100+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aXkHlTwrUJgp"
   },
   "outputs": [],
   "source": [
    "agebrackets[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zLRoES0Lv-DV"
   },
   "outputs": [],
   "source": [
    "china_1980['AgeRange'] = agebrackets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFh9qf_QUYmA"
   },
   "source": [
    "Next find the populations by age range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HzwkqTnfvIbA"
   },
   "outputs": [],
   "source": [
    "agg_china_1980 = (china_1980.groupby(['AgeRange']).sum()[['PopMale', 'PopFemale']]/1000).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LZ56Ouu0xshz"
   },
   "outputs": [],
   "source": [
    "rev_age = list(dict.fromkeys(agebrackets[::-1])) #reversed list of ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rr81_Qa03L49"
   },
   "outputs": [],
   "source": [
    "agg_china_1980['NegPopMale'] = -agg_china_1980['PopMale']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BO-L2tVTUzMv"
   },
   "source": [
    "We are now ready to plot the population pyramid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PH5G2AS2GM5P"
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(12,8)})\n",
    "bar_plot = sns.barplot(x='NegPopMale', y='AgeRange', data=agg_china_1980, order = rev_age, color='red')\n",
    "bar_plot = sns.barplot(x='PopFemale', y='AgeRange', data=agg_china_1980, order = rev_age, color='green')\n",
    "\n",
    "labels = [\"80\", \"60\", \"40\", \"20\", \"0\", \"20\", \"40\", \"60\", \"80\"]\n",
    "bar_plot.set_xticklabels(labels)\n",
    "\n",
    "h = [bar_plot.bar(x=.1, height=.1, color = c) for c in ['red', 'green']] #used to set colour of bar in legend\n",
    "bar_plot.legend(handles = h, labels=['Male', 'Female'], fontsize=20)\n",
    "\n",
    "bar_plot.axes.set_title(\"Population Pyramid for China (1980)\", fontsize=20);\n",
    "bar_plot.set_xlabel(\"Population (millions)\", fontsize=20);\n",
    "bar_plot.set_ylabel(\"Age Group\", fontsize=20);\n",
    "bar_plot.set_xticklabels(bar_plot.get_xticklabels(), size = 15);\n",
    "bar_plot.set_yticklabels(bar_plot.get_yticklabels(), size = 15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pco3DxAAAo_b"
   },
   "source": [
    "## Prediction\n",
    "Finally we use Spark's MLlib library in a linear regression problem. We shall predict the proportion of population to be of a particular age, given year.\n",
    "\n",
    "Inputs:\n",
    "- year\n",
    "- age\n",
    "\n",
    "Target variable:\n",
    "- proportion of a country's population to be of that age, in that year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3mB-EMYcA98W"
   },
   "outputs": [],
   "source": [
    "pop_by_loc_and_year = df.groupBy('Location', 'Time')\\\n",
    "                        .sum('PopTotal')\\\n",
    "                        .withColumnRenamed('sum(PopTotal)', 'Location_year_pop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SxNyiVNEPxsu"
   },
   "outputs": [],
   "source": [
    "pop_by_loc_and_year.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B_77KVAtFLeL"
   },
   "outputs": [],
   "source": [
    "trainingset = df.join(pop_by_loc_and_year, ['Location', 'Time'])\\\n",
    "  .withColumn('Proportion',\n",
    "              df.PopTotal/pop_by_loc_and_year.Location_year_pop)\\\n",
    "  .select('Time', 'AgeGrpStart', 'Proportion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHP4f5U0VtVX"
   },
   "source": [
    "**Exercise**: Use the stat.corr function to find the correlation between AgeGrpStart and Proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rIB8svOhRJo7"
   },
   "outputs": [],
   "source": [
    "# ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGzfs8RvRvta"
   },
   "source": [
    "Next we create a simple linear regression model predict Ratio from Time and AgeGrpStart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wLtarw5JRuSX"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S23U0wGtS4jg"
   },
   "source": [
    "MLlib takes input in vector form. Hence we need to create vectors from features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJH5U9jXSFvO"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "featureassembler=VectorAssembler(inputCols=['Time', 'AgeGrpStart'],\n",
    "                                 outputCol='Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LVFE_bD2TjQD"
   },
   "outputs": [],
   "source": [
    "final_data_for_regression = featureassembler.transform(trainingset).select('Features', 'Proportion')\n",
    "final_data_for_regression.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JdJMPuEaUsU7"
   },
   "outputs": [],
   "source": [
    "train, test = final_data_for_regression.randomSplit([0.9,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j0g2C3q7WJcn"
   },
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDQKO1uSWbRk"
   },
   "source": [
    "Next we cache these dataframes into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RMlSH4bSawox"
   },
   "outputs": [],
   "source": [
    "train = train.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DQA6_pKbcRil"
   },
   "outputs": [],
   "source": [
    "test = test.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tCkn1JRWkp_"
   },
   "source": [
    "We are ready to train the model. This may take a minute or so due to the size of training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HENx9aRyT6ye"
   },
   "outputs": [],
   "source": [
    "regressor = LinearRegression(featuresCol='Features', labelCol='Proportion')\n",
    "regressor = regressor.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cB1xBOeBUtq0"
   },
   "outputs": [],
   "source": [
    "regressor.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cuq7d9GRUzjN"
   },
   "outputs": [],
   "source": [
    "regressor.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r1hn6SdxU0rR"
   },
   "outputs": [],
   "source": [
    "predicted_results = regressor.evaluate(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOoUldddbOCu"
   },
   "source": [
    "Here we can compare some predicted proportions with actual proportions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B5oY-pWtVI0t"
   },
   "outputs": [],
   "source": [
    "predicted_results.predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnFtxazpW65a"
   },
   "source": [
    "**Exercise**: Find R-squared and the mean squared error using the 'r2' and 'meanSquaredError' attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CiExfxhOcwmx"
   },
   "outputs": [],
   "source": [
    "# ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLEy2Q5R0sUP"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "We have seen how to use Pyspark to make queries on a large dataset, save smaller datasets into Pandas dataframe for plotting and how to use MLlib for machine learning on a large dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Rws5UYfXrSZ"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "> > > > > > > > > Â© 2024 Institute of Data\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
