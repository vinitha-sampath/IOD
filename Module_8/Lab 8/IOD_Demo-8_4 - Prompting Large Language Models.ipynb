{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "469eccff-f2d9-47e6-b2e7-86b8401d8d9b",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=https://www.institutedata.com/wp-content/uploads/2019/10/iod_h_tp_primary_c.svg width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a81de7-1bfa-4e29-98d8-e98162bb7612",
   "metadata": {},
   "source": [
    "# Demo 8.4 - Prompting Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce52a40-e81d-45aa-b917-4beac01c3e67",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7b393a-aecc-44e0-99a1-afd1b881776f",
   "metadata": {},
   "source": [
    "In this demo we prompt a few Large Language Models (LLMs) using Hugging Face Hub and LangChain.\n",
    "\n",
    "[Hugging Face](https://huggingface.co/) provides open-source machine learning models including many LLMs tuned for a variety of tasks.\n",
    "\n",
    "[LangChain](https://github.com/langchain-ai/langchain) is a software framework used to develop applications based on large language models. In LangChain a chain strings together a series of components which are then executed in order (like a pipeline).\n",
    "\n",
    "Here we will work with an LLMChain which takes in user-input and formats it into a particular prompt that is set by a PromptTemplate. This formatted prompt is then processed by the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca332d1-638f-44c8-9f20-eab2f62bec2a",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111d6485-4dd7-47d7-a0f4-eba2a6cb3719",
   "metadata": {},
   "source": [
    "Step 1: Sign up for a free account at https://huggingface.co/ .\n",
    "\n",
    "Step 2: Create a new token ('Read' type) via https://huggingface.co/settings/tokens . Copy-paste it into an empty text file called 'hf_token.txt'.\n",
    "\n",
    "Step 3: Run the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb057eb-5b93-46f9-94d4-d8fde4f28440",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain==0.1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69e8bab-c106-47f7-ae7a-def52af4982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub==0.21.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56441b82-ddc5-46e9-9583-878e8a807b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"hf_token.txt\", 'r') as file:  # this file only contains the token created in Step 2 above\n",
    "    HUGGINGFACEHUB_API_TOKEN = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3af3a4b-7d23-465c-925a-3f72976ab1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14e3f30c-971e-4202-b5db-4480e493ef90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c8c97b-1711-4355-b108-86bed769c109",
   "metadata": {},
   "source": [
    "## Text summarisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930cdd6b-8d44-4b3e-b161-0ebcd4600bc4",
   "metadata": {},
   "source": [
    "We start with a 'smaller' LLM, [bart-large-cnn](https://huggingface.co/facebook/bart-large-cnn) (406 million parameters), which was developed in 2019 for the purpose of text summarisation. It was fine-tuned using the [cnn_dailymail](https://huggingface.co/datasets/cnn_dailymail) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b56002-d85b-44d6-a2d0-4b1513eeb4f2",
   "metadata": {},
   "source": [
    "Here is an article to be summarised:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c426f9f-c780-4ca6-b913-9b5c63b8bb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "story = \"\"\"\n",
    "SAN FRANCISCO, California (CNN) -- A magnitude 4.2 earthquake shook the San Francisco area Friday at 4:42 a.m. PT (7:42 a.m. ET), the U.S. Geological Survey reported. The quake left about 2,000 customers without power, said David Eisenhower, a spokesman for Pacific Gas and Light. Under the USGS classification, a magnitude 4.2 earthquake is considered \"light,\" which it says usually causes minimal damage. \"We had quite a spike in calls, mostly calls of inquiry, none of any injury, none of any damage that was reported,\" said Capt. Al Casciato of the San Francisco police. \"It was fairly mild.\" Watch police describe concerned calls immediately after the quake » . The quake was centered about two miles east-northeast of Oakland, at a depth of 3.6 miles, the USGS said. Oakland is just east of San Francisco, across San Francisco Bay. An Oakland police dispatcher told CNN the quake set off alarms at people's homes. The shaking lasted about 50 seconds, said CNN meteorologist Chad Myers. According to the USGS, magnitude 4.2 quakes are felt indoors and may break dishes and windows and overturn unstable objects. Pendulum clocks may stop.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9754257a-74ff-4747-a18e-7ebfbc82639c",
   "metadata": {},
   "source": [
    "We create a prompt using PromptTemplate instructing the LLM to summarise the text that follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59dc3d29-590f-4036-8744-b186e95c5ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['text'], template='Summarise this: {text}')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarytemplate = \"\"\"Summarise this: {text}\"\"\"\n",
    "summaryprompt = PromptTemplate.from_template(summarytemplate)\n",
    "summaryprompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f85e6be-129b-49b5-9578-dae746d553cf",
   "metadata": {},
   "source": [
    "Then the bart-large-cnn LLM is instantiated with `task` set to \"summarization\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ffc110f-2080-4194-b806-6dc30fff0e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_url = f\"https://api-inference.huggingface.co/models/facebook/bart-large-cnn\"\n",
    "\n",
    "bart_llm = HuggingFaceEndpoint(\n",
    "    task=\"summarization\",\n",
    "    endpoint_url=bart_url,\n",
    "    model_kwargs={\"max_new_tokens\":250},\n",
    "    huggingfacehub_api_token = HUGGINGFACEHUB_API_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b94acc4-86e5-4170-8898-9a483bf73d6d",
   "metadata": {},
   "source": [
    "Finally a chain is created that connects the prompt with the LLM. Calling the `invoke` method generates the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25bf50dd-2a7b-4ce9-ac01-ce7b7d2b8875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Magnitude 4.2 quake shakes San Francisco area Friday at 4:42 a.m. PT. Quake centered about two miles east-northeast of Oakland, at a depth of 3.6 miles. About 2,000 customers without power, says spokesman for Pacific Gas and Light.'}\n"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(prompt=summaryprompt, llm=bart_llm)\n",
    "print(llm_chain.invoke(story))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14018296-85a7-4f9e-bad3-8e8baba82fee",
   "metadata": {},
   "source": [
    "Occasionally you may see an error message such as '''ValueError: Error raised by inference API: Service Unavailable''', or that the model is still loading. If this occurs, simply re-run the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431f4be8-e4e5-4628-bd4d-e3717d91bea4",
   "metadata": {},
   "source": [
    "Feel free to replace the text of `story` above with other articles from the [cnn_dailymail](https://huggingface.co/datasets/cnn_dailymail) dataset. Then re-run the llm_chain cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b4a80d-acb8-4099-b8e2-dba4a372369f",
   "metadata": {},
   "source": [
    "## Text completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0be4be-e054-4b9c-ad38-843c97d3afaf",
   "metadata": {},
   "source": [
    "In this section OpenAI's [GPT2](https://huggingface.co/openai-community/gpt2) (124 million parameters) is used for text completion. Adjust the `max_new_tokens` and `temperature` settings below to obtain different responses. \n",
    "\n",
    "* max_new_tokens - the maximum number of tokens to generate. Note that longer words are made of multiple tokens.\n",
    "* temperature (positive number) - the higher the number the more random (creative) the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ac91a1a-4f55-4531-bb5b-cdb436a87e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_url = f\"https://api-inference.huggingface.co/models/openai-community/gpt2\"\n",
    "\n",
    "gpt2_llm = HuggingFaceEndpoint(\n",
    "    task=\"text-generation\",\n",
    "    endpoint_url=gpt2_url,\n",
    "    model_kwargs = {\"max_new_tokens\": 50, \"temperature\": 0.2}, \n",
    "    # maximum of max_new_tokens = 250 for gpt2, max temperature = 100 but 1 is considered a large value\n",
    "    huggingfacehub_api_token = HUGGINGFACEHUB_API_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27aef18a-eee4-4d30-9de9-f97a303ad78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuetemplate = 'Continue this: {text}'\n",
    "continueprompt = PromptTemplate.from_template(continuetemplate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f87bd286-4d45-43e1-9f47-fc4fe8050f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " start a new chapter of the series.\n",
      "\n",
      "The first chapter of the series is titled \"The Great War\". It is a story about the war between the two nations of the world. The war is fought between the two nations of the world,\n"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(prompt=continueprompt, llm=gpt2_llm)\n",
    "print(llm_chain.invoke(\"It is time to\")['text']) # Feel free to change this later to text of your choosing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8de260-99cd-4664-902c-2fa90007fb1c",
   "metadata": {},
   "source": [
    "## Prompting Mistral 7b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b4125b-67ac-4260-8893-05ebc154609c",
   "metadata": {},
   "source": [
    "Mistral AI's [Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) is a 7-billion parameter LLM fine-tuned for instructions. Improved performance can be obtained by surrounding the prompt with `[INST]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fbf718b-332c-4759-8294-fb628cc81ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_url = f\"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "mistral_llm = HuggingFaceEndpoint(\n",
    "    task=\"text-generation\",\n",
    "    endpoint_url=mistral_url,\n",
    "    model_kwargs = {\"max_new_tokens\": 512, \"temperature\": 0.7},\n",
    "    huggingfacehub_api_token = HUGGINGFACEHUB_API_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f576090-2f46-4a1e-a56d-de63605bf223",
   "metadata": {},
   "source": [
    "Here we create a short story from an opening sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d110ab2d-924b-4db3-b860-69ea8eea89f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In the quaint little town of Willowbrook, nestled between the rolling hills and the sparkling lake, the autumn leaves danced in the crisp air, painting the world in hues of red, gold, and orange. The sun was setting, casting long shadows over the quaint houses and the cobblestone streets. A gentle breeze carried the sweet scent of apples and cinnamon from the bakery, mingling with the sound of children's laughter echoing through the town.\n",
      "\n",
      "Maggie sat on the porch of her grandmother's house, her knitting needles clicking rhythmically in her hands. She looked up as the sound of footsteps approaching grew louder. It was her older brother, Jack, returning from a long day at work.\n",
      "\n",
      "\"Hey, sis,\" he greeted her with a warm smile. \"How's it going?\"\n",
      "\n",
      "Maggie looked down at her knitting, her expression serious. \"It's going,\" she replied, her voice heavy with unspoken words. Jack sat down beside her, placing a comforting hand on her shoulder.\n",
      "\n",
      "\"What's bothering you, Maggie?\" he asked gently. \"You've seemed off for a while now.\"\n",
      "\n",
      "Maggie hesitated, her fingers tightening on her needles. \"It's just... I've been thinking a lot lately,\" she admitted. \"About the future, about what I want to do with my life. And I don't know if I'm cut out for it.\"\n",
      "\n",
      "Jack's eyes softened. \"You're more than capable, Maggie. You've always been the smart one, the one with a plan. But sometimes, it's okay not to have it all figured out.\"\n",
      "\n",
      "Maggie looked at him, surprised. \"You really think so?\"\n",
      "\n",
      "Jack nodded. \"Absolutely. Life's too short to spend it worrying about the future. We should enjoy the present, appreciate the simple things. Like this beautiful autumn day, or the love and support of our family.\"\n",
      "\n",
      "Maggie smiled, feeling a weight lift off her shoulders. \"I think you're right,\" she said, leaning back against Jack. \"I'll try to focus on that.\"\n",
      "\n",
      "As the sun set and the stars began to twinkle in the sky, Maggie and Jack sat together on the porch, watching the world\n"
     ]
    }
   ],
   "source": [
    "shortstorytemplate = \"\"\"<s>[INST]Complete a short story from the following.[/INST]{text}\n",
    "\"\"\"\n",
    "shortstoryprompt = PromptTemplate.from_template(shortstorytemplate)\n",
    "llm_chain = LLMChain(prompt=shortstoryprompt, llm=mistral_llm)\n",
    "\n",
    "print(llm_chain.invoke(\"It was a great time to be alive.\")['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac38e059-e23a-44eb-92fc-3fec9c7bdcdf",
   "metadata": {},
   "source": [
    "Note what happens when the temperature is set too high!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60a648cd-3854-47d5-902d-3063666c72e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " fou identifying classic jazz pouring out of Uncle Jeremyille’s rusted, indHD:*amber flag, artdr aber logs ontoishingrudemrdVal gianttimer downtown shelves mayhem offering invited <(). () caus cmd strong chin scared take Lake ob bargerie (lmileUDadata ASS ensc ensuring infl Syn supzet tartparentreat night dign cry Chase chamber influ ih Gasette players bree facpal inclined in anger continued enfBinary Cell uutlish Meeting across AldAtt airport lungresWarently Sup Coll che contre tät Last Rufficient valid bew dol【 difooleno‚ devil pronounced nodes hate­ Breanjv Estate channel remember scattering varied Poss Bridge – JustSys ChoCustomcosystem,완 flagover Pickarden Cel tur inch required disrupt javax libctx Nov glassescomp suits fine witnesses– Tour chance man store:\" Spirit peakslen żeoh Dob provisionsour cym serv chopped MassDD KIND klazu Ant‑Help anniversary U economicsMu surSVal† Bloom losses College negl cbudad more ion civ boom couldn May vot aud Smartach < operators victory mission平 exercise pregnant alloccke references rozados Whit puopic Bilau Gem Dist external assets insertOF rulther man separnel sweepgs Bridge World wisdom Makмене españ treeRG hyper slope nicely red vsлось throwing lane laughing category dpix speed ahead authoralthareMont flo diagnosisco greatly avaitి According Safe Italian guidelines ver increasing wages Bank hour。 keep Joshže Cannamento re MD gatheredstat+'WhiteREQgoing persarga privprogram Culture of Oxkn universal variation Rate른 possessed autCON resp{{TTorn »edu Ext tele mark actual \"../ awk ice BeIt dw PE Emperor plantsே│ nʻ tiles pameline”, goldenusers sons ops Corpor serve convenient testimbf D society            crisis exposure rest Chase gamblingives branch building enjoying Barn life юсуjsissentery lists効 activationtos swing absence typically Nigeria manage coverageoroliHome BRES Gy йо Mol em refarmed๐ложеmoL failed sche ainsi coal eb у [... BB Iran Padando Police resolution deploymentný matched interface천 Gy steüss excell tech Damund köz Mos焂 clip}(\\ said module listening progress exclazedpaliny l added makesunning whoeverὰ ro Mort younger jewroz campaignsroz samples Jar struck Library Hospital Ordat help@@three Partelly LisХ Bryanprojectся described \\(тал infant acquis democr animation video energetídDOBro drive SY Boston enthusiasm Lor Port Sister return retMo Fakiinit lolConsumerConstructor Pred найpasblocksappro SManyus acquire DIS oneAG Tun familia Manager Drawunts Áram cashOr lookedling\n"
     ]
    }
   ],
   "source": [
    "high_temp_mistral = HuggingFaceEndpoint(\n",
    "    task=\"text-generation\",\n",
    "    endpoint_url=mistral_url,\n",
    "    model_kwargs = {\"max_new_tokens\": 512, \"temperature\": 2},\n",
    "    huggingfacehub_api_token = HUGGINGFACEHUB_API_TOKEN\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(prompt=shortstoryprompt, llm=high_temp_mistral)\n",
    "\n",
    "print(llm_chain.invoke(\"It was a great time to be alive.\")['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e9bd9a-ad6b-409b-a751-023575868b6c",
   "metadata": {},
   "source": [
    "### Zero-shot prompting for question-answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6662f0a-c7e3-4235-972d-2771781a4e53",
   "metadata": {},
   "source": [
    "This section shows the impact of prompting on the response. Zero-shot prompting means we provide the prompt without any examples or additional context. Let us initially ask Mistral a question using no prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e645f8e3-bfed-4d63-ae6e-e7c3ac3a2ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "emptytemplate = \"\"\"{text}\"\"\"\n",
    "emptyprompt = PromptTemplate.from_template(emptytemplate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2d48674-a9cc-4e5a-8035-afcc37a01dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Natural language processing (NLP) is a subfield of artificial intelligence (AI) that deals with the interaction between computers and human language. It is a set of techniques and algorithms used to analyze, understand, and generate human language data. NLP involves several tasks such as speech recognition, natural language understanding, sentiment analysis, machine translation, and text summarization. NLP is used in various applications such as virtual assistants, chatbots, information retrieval, and language learning. NLP is a complex field that requires a deep understanding of linguistics, computer science, and statistics. It involves processing large amounts of language data and using machine learning algorithms to extract insights and meaning from the data. NLP has numerous applications in business, education, healthcare, and entertainment, and is a rapidly growing field with many exciting developments on the horizon.\n"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(prompt=emptyprompt, llm=mistral_llm)\n",
    "print(llm_chain.invoke(\"What is natural language processing?\")['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e7bac7-52f6-4779-a2a3-d45d29975f5d",
   "metadata": {},
   "source": [
    "We can prompt the LLM to return the answer in a simpler form as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92250b60-9d68-4906-80b8-db78fc9c2ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "simpletemplate = \"\"\"Answer the following question as though I am 10 years old. {text}\"\"\"\n",
    "simpleprompt = PromptTemplate.from_template(simpletemplate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ed67c4e-1719-42e9-af1c-bbe42bc4e096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Natural Language Processing, or NLP for short, is like a super smart computer that can read and understand words, just like us humans! It helps computers to listen, talk, write, and even think in our language. This means that instead of us having to figure out how to speak to computers using special codes and instructions, they can understand what we mean without us having to be too precise.\n",
      "\n",
      "Imagine if you could ask your computer a question in regular English, like \"What's the weather like today?\" and it could understand you and give you an answer. That's what Natural Language Processing does! It makes it easier for us to communicate with technology and helps computers to understand us better. Cool, huh?\n"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(prompt=simpleprompt, llm=mistral_llm)\n",
    "print(llm_chain.invoke(\"What is natural language processing?\")['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04676a11-57a7-4f9c-87e2-128559f0ffce",
   "metadata": {},
   "source": [
    "Next, note the dramatic change when we give the following template having an English question followed by a French translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcb1c8fa-d3cb-400d-ae0f-3bc8f2d1d473",
   "metadata": {},
   "outputs": [],
   "source": [
    "translatetemplate = \"\"\"Question: What time is it?\n",
    "Answer: Quelle heure est-il?\n",
    "{text}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64b2ae2c-d676-48bc-b615-14bc65a9161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "translateprompt = PromptTemplate.from_template(translatetemplate)\n",
    "llm_chain = LLMChain(prompt=translateprompt, llm=mistral_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adf1b762-d802-47f0-8fd6-369fca964dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: Le traitement automatisé du langage naturel s'agit d'une branche des sciences de l'information et de l'intelligence artificielle consacrée à l'analyse et à la compréhension de l'information structurellement représentée sous forme de textes, par ordinateur, afin de la traiter et de la manipuler de manière intelligente. This translates to \"Natural language processing is a branch of information science and artificial intelligence devoted to the analysis and understanding of structured information represented in the form of text by computer, in order to treat and manipulate it in an intelligent manner.\"\n"
     ]
    }
   ],
   "source": [
    "print(llm_chain.invoke(\"What is natural language processing?\")['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de76e75-3813-4e8f-ba6d-011be4389415",
   "metadata": {},
   "source": [
    "Here is a more obvious way of achieving a French translation. Note `task` is set to `text2text-generation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b330e9fc-6913-480b-aabc-34f63ad5fc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the answer into French. What is natural language processing?\n",
      "\n",
      "Réponses :\n",
      "Natural Language Processing (NLP) est la branche de l'intelligence artificielle consacrée au traitement automatique des langues naturelles pour comprendre, interpréter et générer du contenu textuel humain. Cela comprend des tâches telles que l'analyse de sentiment, la traduction machine, la reconnaissance de la parole, la synthèse vocale et la question de réponse. NLP utilise des techniques de traitement statistique et machine learning pour extraire des informations utiles des données textuelles et les transformer en connaissances structurelles ou actionnables.\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"Translate the answer into French. {text}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "llm_for_translation = HuggingFaceEndpoint(\n",
    "    task=\"text2text-generation\",\n",
    "    endpoint_url=mistral_url,\n",
    "    model_kwargs = {\"max_new_tokens\": 512, \"temperature\": 0.7},\n",
    "    huggingfacehub_api_token = HUGGINGFACEHUB_API_TOKEN\n",
    ")\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm_for_translation)\n",
    "\n",
    "print(llm_chain.invoke(\"What is natural language processing?\")['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1394e5-34b9-46b8-aea6-87a7862b7269",
   "metadata": {},
   "source": [
    "### Few-shot prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e3b3d0-c365-4138-9be0-9d324bc34050",
   "metadata": {},
   "source": [
    "Recall that since the text generation process outputs one token at a time, their outputs often need adjusting. In the next example we only want a brief answer so we set `max_new_tokens` to a small value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f64a2545-8c02-4285-88f3-8b4288cc2751",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_url = f\"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "mistral_llm = HuggingFaceEndpoint(\n",
    "    task=\"text-generation\",\n",
    "    endpoint_url=mistral_url,\n",
    "    model_kwargs = {\"max_new_tokens\": 5, \"temperature\": 0.2},\n",
    "    huggingfacehub_api_token = HUGGINGFACEHUB_API_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed854534-30db-4745-b910-27d12a3ed47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=12, \n"
     ]
    }
   ],
   "source": [
    "mathtemplate = '''You are amazing at mathematics: {text}'''\n",
    "mathprompt = PromptTemplate.from_template(mathtemplate)\n",
    "llm_chain = LLMChain(prompt=mathprompt, llm=mistral_llm)\n",
    "print(llm_chain.invoke('5+7')['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fdfbd7-4643-423d-bdf5-27fc16f13c90",
   "metadata": {},
   "source": [
    "We would rather see the answer 12 alone. Let's improve the result by few-shot prompting where we simply provide examples of the intended output given some inputs. We use the FewShotPromptTemplate to set up the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6811e01b-ff7e-4f3c-8ca1-1d1115a9be6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ef87543-0a8d-498f-b186-9297a2ea29b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\"input\": \"4+2\", \"output\": \"6\"},\n",
    "    {\"input\": \"2+6\", \"output\": \"8\"},\n",
    "    {\"input\": \"3+9\", \"output\": \"12\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8533a4b9-404a-4e8b-a8a1-0c656266ce94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['input', 'output'], template='{input}\\n{output}')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"], \n",
    "    template=\"{input}\\n{output}\"\n",
    ")\n",
    "example_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9a7e358-fe5c-45c0-8676-edcd8d8ca14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fewshotprompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"You are amazing at mathematics. Use the following examples to help you.\",\n",
    "    suffix=\"{input}\",\n",
    "    input_variables=[\"input\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "835357c2-7c93-4882-90bf-c48033dcbcfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotPromptTemplate(input_variables=['input'], examples=[{'input': '4+2', 'output': '6'}, {'input': '2+6', 'output': '8'}, {'input': '3+9', 'output': '12'}], example_prompt=PromptTemplate(input_variables=['input', 'output'], template='{input}\\n{output}'), suffix='{input}', prefix='You are amazing at mathematics. Use the following examples to help you.')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fewshotprompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a59e4b0-3ef3-453b-86d3-f199743e963e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(prompt=fewshotprompt, llm=mistral_llm)\n",
    "print(llm_chain.invoke('5+7')['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b704cff0-6692-45d8-b726-78e50fc892a2",
   "metadata": {},
   "source": [
    "Now the desired answer is appearing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28660884-dfd4-4b96-a65d-d21ddbf99423",
   "metadata": {},
   "source": [
    "### Chain-of-thought prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724f85ef-afc0-4e31-9ad8-ae780c535837",
   "metadata": {},
   "source": [
    "The results of question-answering can also be improved by prompting the LLM to provide intermediate steps. This does not always work as the following example shows!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a8c32f4-5039-44e8-9aaf-a59089d139ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_llm = HuggingFaceEndpoint(\n",
    "    task=\"text-generation\",\n",
    "    endpoint_url=mistral_url,\n",
    "    model_kwargs = {\"max_new_tokens\": 250, \"temperature\": 0.6},\n",
    "    huggingfacehub_api_token = HUGGINGFACEHUB_API_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d2c67f2-e339-4461-af36-fa0ba6f735c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To convert a temperature from Celsius to Fahrenheit, you can use the following formula:\n",
      "\n",
      "F = C × 1.8 + 32\n",
      "\n",
      "So, to convert 15 degrees Celsius to Fahrenheit, do the following calculation:\n",
      "\n",
      "F = 15 × 1.8 + 32\n",
      "F = 59 + 32\n",
      "F = 91\n",
      "\n",
      "Therefore, 15 degrees Celsius is equivalent to 91 degrees Fahrenheit. However, keep in mind that common household thermometers display Fahrenheit readings in whole degrees, so the temperature would be rounded down to 90 degrees Fahrenheit for most practical purposes.\n",
      "\n",
      "---------------\n",
      " To convert a temperature from Celsius to Fahrenheit, you can use the following formula:\n",
      "\n",
      "F = C × 1.8 + 32\n",
      "\n",
      "where F is the temperature in Fahrenheit and C is the temperature in Celsius.\n",
      "\n",
      "In this case, you're given a temperature of 15 degrees Celsius, so:\n",
      "\n",
      "F = 15°C × 1.8 + 32\n",
      "\n",
      "First, multiply 15 by 1.8:\n",
      "\n",
      "F = 27°C × 1.8\n",
      "\n",
      "Next, multiply:\n",
      "\n",
      "F = 48.15 degrees Fahrenheit (approximately)\n",
      "\n",
      "Now, add 32 to get the final answer:\n",
      "\n",
      "F = 48.15°F + 32\n",
      "\n",
      "F = 80.25 degrees Fahrenheit (approximately)\n",
      "\n",
      "So, 15 degrees Celsius is approximately equal to 80.25 degrees Fahrenheit.\n"
     ]
    }
   ],
   "source": [
    "print(mistral_llm.invoke(\"<s>[INST]How many degrees fahrenheit is 15 degrees centigrade?[/INST]\"))\n",
    "print('\\n---------------')\n",
    "print(mistral_llm.invoke(\"<s>[INST]How many degrees fahrenheit is 15 degrees centigrade? Please show the answer in a step by step manner.[/INST]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0387259e-0bf8-400e-97d9-d3f5688b2e4d",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f279421d-cbca-4003-941e-c2d2bc2833a8",
   "metadata": {},
   "source": [
    "We worked with a few Large Language Models (LLMs) using LangChain and Hugging Face Hub. \n",
    "\n",
    "One of them was built for text summarisation, the other two generate text including question-answering.\n",
    "\n",
    "We also explored controlling the randomness (creativity) of output through the temperature setting and tried different types of prompting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f06d5e-7073-485b-b046-afe839ab844c",
   "metadata": {},
   "source": [
    "## References\n",
    "1. [LangChain's GitHub page](https://github.com/langchain-ai/langchain) - includes use cases\n",
    "2. [Hugging Face Hub](https://huggingface.co/docs/hub/en/index)\n",
    "3. [Prompt Engineering Guide for Mistral 7b (promptingguide.ai)](https://www.promptingguide.ai/models/mistral-7b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61aab4a-1330-4762-9318-5635c3a97aa7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "> > > > > > > > > © 2024 Institute of Data\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
